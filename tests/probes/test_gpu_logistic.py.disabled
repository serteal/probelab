"""
Tests for GPU-based logistic regression probe.
"""

import pytest
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

from probelib.probes.gpu_logistic import GPULogistic, LogisticNetwork, GPUStandardScaler
from probelib.processing.activations import Activations
from probelib.types import Label


class TestGPUStandardScaler:
    """Test GPU-based standardization."""

    def test_fit_transform(self):
        """Test that GPU scaler matches sklearn StandardScaler."""
        # Create random data
        X = torch.randn(100, 50)

        # sklearn scaler
        sklearn_scaler = StandardScaler()
        X_sklearn = sklearn_scaler.fit_transform(X.numpy())

        # GPU scaler
        device = "cuda" if torch.cuda.is_available() else "cpu"
        gpu_scaler = GPUStandardScaler(device=device)
        X_gpu = gpu_scaler.fit_transform(X)

        # Compare results
        np.testing.assert_allclose(X_gpu.cpu().numpy(), X_sklearn, rtol=1e-5, atol=1e-6)

        # Test mean and std
        np.testing.assert_allclose(
            gpu_scaler.mean_.cpu().numpy(), sklearn_scaler.mean_, rtol=1e-5, atol=1e-6
        )
        np.testing.assert_allclose(
            gpu_scaler.scale_.cpu().numpy(), sklearn_scaler.scale_, rtol=1e-5, atol=1e-6
        )

    def test_partial_fit(self):
        """Test incremental fitting with partial_fit."""
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # Create data in batches
        batch1 = torch.randn(50, 30)
        batch2 = torch.randn(50, 30)
        batch3 = torch.randn(50, 30)

        # Fit all at once
        all_data = torch.cat([batch1, batch2, batch3], dim=0)
        scaler_batch = GPUStandardScaler(device=device)
        scaler_batch.fit(all_data)

        # Fit incrementally
        scaler_streaming = GPUStandardScaler(device=device)
        scaler_streaming.partial_fit(batch1)
        scaler_streaming.partial_fit(batch2)
        scaler_streaming.partial_fit(batch3)

        # Compare statistics (should be very close)
        torch.testing.assert_close(
            scaler_streaming.mean_, scaler_batch.mean_, rtol=1e-4, atol=1e-5
        )
        torch.testing.assert_close(
            scaler_streaming.var_, scaler_batch.var_, rtol=1e-3, atol=1e-4
        )

    def test_transform_before_fit(self):
        """Test that transform raises error before fitting."""
        device = "cuda" if torch.cuda.is_available() else "cpu"
        scaler = GPUStandardScaler(device=device)
        X = torch.randn(10, 5)

        with pytest.raises(RuntimeError, match="must be fitted"):
            scaler.transform(X)


class TestLogisticNetwork:
    """Test the LogisticNetwork module."""

    def test_forward_shape(self):
        """Test that forward pass returns correct shape."""
        d_model = 768
        batch_size = 32

        network = LogisticNetwork(d_model)
        X = torch.randn(batch_size, d_model)

        logits = network(X)
        assert logits.shape == (batch_size,)

    def test_no_bias(self):
        """Test that network has no bias term."""
        network = LogisticNetwork(100)
        assert network.linear.bias is None

    def test_gradient_flow(self):
        """Test that gradients flow through the network."""
        network = LogisticNetwork(50)
        X = torch.randn(10, 50, requires_grad=True)
        y = torch.randint(0, 2, (10,)).float()

        logits = network(X)
        loss = torch.nn.functional.binary_cross_entropy_with_logits(logits, y)
        loss.backward()

        # Check that gradients exist
        assert network.linear.weight.grad is not None
        assert X.grad is not None


class TestGPULogistic:
    """Test the GPU logistic regression probe."""

    @pytest.fixture
    def sample_activations(self):
        """Create sample activations for testing."""
        batch_size = 20
        seq_len = 50
        d_model = 128

        # Create activations with some detected tokens
        detection_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)
        detection_mask[:, 10:30] = True  # Mark middle tokens as detected

        return Activations(
            activations=torch.randn(1, batch_size, seq_len, d_model),
            attention_mask=torch.ones(batch_size, seq_len),
            input_ids=torch.randint(0, 1000, (batch_size, seq_len)),
            detection_mask=detection_mask,
            layer_indices=[12],
        )

    def test_sequence_aggregation_fit(self, sample_activations):
        """Test fitting with sequence aggregation."""
        probe = GPULogistic(layer=12, sequence_aggregation="mean", verbose=True)

        # Create labels
        labels = [
            Label.POSITIVE if i % 2 == 0 else Label.NEGATIVE
            for i in range(sample_activations.batch_size)
        ]

        # Fit the probe
        probe.fit(sample_activations, labels)

        assert probe._fitted
        assert probe._network is not None
        assert probe._scaler.mean_ is not None

    def test_score_aggregation_fit(self, sample_activations):
        """Test fitting with score aggregation (token-level)."""
        probe = GPULogistic(layer=12, score_aggregation="mean", verbose=True)

        # Create labels
        labels = [
            Label.POSITIVE if i % 2 == 0 else Label.NEGATIVE
            for i in range(sample_activations.batch_size)
        ]

        # Fit the probe
        probe.fit(sample_activations, labels)

        assert probe._fitted
        assert probe._network is not None
        assert hasattr(probe, "_tokens_per_sample")

    def test_predict_proba_shape(self, sample_activations):
        """Test that predict_proba returns correct shape."""
        probe = GPULogistic(layer=12, sequence_aggregation="mean")

        labels = [Label.POSITIVE] * sample_activations.batch_size
        probe.fit(sample_activations, labels)

        probs = probe.predict_proba(sample_activations)

        assert probs.shape == (sample_activations.batch_size, 2)
        assert torch.allclose(
            probs.sum(dim=1), torch.ones(probs.shape[0], device=probs.device)
        )
        assert (probs >= 0).all() and (probs <= 1).all()

    def test_partial_fit(self, sample_activations):
        """Test incremental fitting with partial_fit."""
        probe = GPULogistic(layer=12, sequence_aggregation="mean")

        # Split into batches
        batch_size = sample_activations.batch_size // 2

        batch1 = Activations(
            activations=sample_activations.activations[:, :batch_size],
            attention_mask=sample_activations.attention_mask[:batch_size],
            input_ids=sample_activations.input_ids[:batch_size],
            detection_mask=sample_activations.detection_mask[:batch_size],
            layer_indices=[12],
        )

        batch2 = Activations(
            activations=sample_activations.activations[:, batch_size:],
            attention_mask=sample_activations.attention_mask[batch_size:],
            input_ids=sample_activations.input_ids[batch_size:],
            detection_mask=sample_activations.detection_mask[batch_size:],
            layer_indices=[12],
        )

        labels1 = [Label.POSITIVE] * batch_size
        labels2 = [Label.NEGATIVE] * (sample_activations.batch_size - batch_size)

        # Partial fit
        probe.partial_fit(batch1, labels1)
        assert probe._fitted

        probe.partial_fit(batch2, labels2)
        assert probe._fitted

        # Should be able to predict
        probs = probe.predict_proba(sample_activations)
        assert probs.shape[0] == sample_activations.batch_size

    def test_different_optimizers(self, sample_activations):
        """Test that different optimizers work."""
        labels = [
            Label.POSITIVE if i % 2 == 0 else Label.NEGATIVE
            for i in range(sample_activations.batch_size)
        ]

        for optimizer_type in ["sgd", "adam", "adamw"]:
            probe = GPULogistic(
                layer=12,
                sequence_aggregation="mean",
                optimizer_type=optimizer_type,
                n_epochs=5,  # Quick test
            )
            probe.fit(sample_activations, labels)
            assert probe._fitted

            probs = probe.predict_proba(sample_activations)
            assert probs.shape == (sample_activations.batch_size, 2)

    def test_empty_batch_handling(self):
        """Test handling of empty batches (no detected tokens)."""
        batch_size = 4
        seq_len = 20
        d_model = 64

        # Create activations with no detected tokens
        empty_activations = Activations(
            activations=torch.randn(1, batch_size, seq_len, d_model),
            attention_mask=torch.ones(batch_size, seq_len),
            input_ids=torch.randint(0, 1000, (batch_size, seq_len)),
            detection_mask=torch.zeros(batch_size, seq_len, dtype=torch.bool),
            layer_indices=[12],
        )

        probe = GPULogistic(layer=12, score_aggregation="mean", verbose=True)
        labels = [Label.POSITIVE] * batch_size

        # Should handle empty batch gracefully
        probe.partial_fit(empty_activations, labels)
        assert not probe._fitted  # Should not be fitted with empty batch

    def test_save_load(self, sample_activations, tmp_path):
        """Test saving and loading the probe."""
        probe = GPULogistic(
            layer=12,
            sequence_aggregation="mean",
            l2_penalty=0.5,
            optimizer_type="adam",
        )

        labels = [
            Label.POSITIVE if i % 2 == 0 else Label.NEGATIVE
            for i in range(sample_activations.batch_size)
        ]
        probe.fit(sample_activations, labels)

        # Save
        save_path = tmp_path / "gpu_logistic.pt"
        probe.save(save_path)

        # Load
        loaded_probe = GPULogistic.load(save_path)

        # Check attributes
        assert loaded_probe.layer == probe.layer
        assert loaded_probe.l2_penalty == probe.l2_penalty
        assert loaded_probe.optimizer_type == probe.optimizer_type
        assert loaded_probe._fitted

        # Check predictions are the same
        probs1 = probe.predict_proba(sample_activations)
        probs2 = loaded_probe.predict_proba(sample_activations)
        torch.testing.assert_close(probs1, probs2)

    def test_regularization_effect(self, sample_activations):
        """Test that L2 regularization has an effect."""
        labels = [
            Label.POSITIVE if i % 2 == 0 else Label.NEGATIVE
            for i in range(sample_activations.batch_size)
        ]

        # Train with no regularization
        probe_no_reg = GPULogistic(
            layer=12,
            sequence_aggregation="mean",
            l2_penalty=0.0,
            n_epochs=50,
        )
        probe_no_reg.fit(sample_activations, labels)

        # Train with strong regularization
        probe_strong_reg = GPULogistic(
            layer=12,
            sequence_aggregation="mean",
            l2_penalty=10.0,
            n_epochs=50,
        )
        probe_strong_reg.fit(sample_activations, labels)

        # Weights should be smaller with regularization
        weights_no_reg = probe_no_reg._network.linear.weight.abs().mean()
        weights_strong_reg = probe_strong_reg._network.linear.weight.abs().mean()

        assert weights_strong_reg < weights_no_reg

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_gpu_device_handling(self, sample_activations):
        """Test that probe works correctly on GPU."""
        probe = GPULogistic(
            layer=12,
            sequence_aggregation="mean",
            device="cuda",
        )

        labels = [Label.POSITIVE] * sample_activations.batch_size
        probe.fit(sample_activations, labels)

        assert probe._network.linear.weight.device.type == "cuda"
        assert probe._scaler.mean_.device.type == "cuda"

        probs = probe.predict_proba(sample_activations)
        assert probs.device.type == "cuda"


class TestGPUvsSklearn:
    """Compare GPU implementation with sklearn for correctness."""

    def test_similar_results_to_sklearn(self):
        """Test that GPU logistic gives similar results to sklearn."""
        # Create simple dataset
        n_samples = 100
        n_features = 20
        X = torch.randn(n_samples, n_features)
        y = torch.randint(0, 2, (n_samples,))

        # sklearn logistic regression
        sklearn_lr = LogisticRegression(
            penalty="l2",
            C=1.0,  # C = 1/l2_penalty
            fit_intercept=False,
            max_iter=100,
            random_state=42,
        )
        X_scaled = StandardScaler().fit_transform(X.numpy())
        sklearn_lr.fit(X_scaled, y.numpy())
        sklearn_probs = sklearn_lr.predict_proba(X_scaled)

        # GPU logistic regression
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # Create mock activations
        activations = Activations(
            activations=X.unsqueeze(0).unsqueeze(2),  # [1, n_samples, 1, n_features]
            attention_mask=torch.ones(n_samples, 1),
            input_ids=torch.ones(n_samples, 1, dtype=torch.long),
            detection_mask=torch.ones(n_samples, 1, dtype=torch.bool),
            layer_indices=[0],
        )

        gpu_lr = GPULogistic(
            layer=0,
            sequence_aggregation="mean",  # Will just extract the single token
            l2_penalty=1.0,
            optimizer_type="sgd",
            learning_rate=0.1,
            n_epochs=100,
            device=device,
            random_state=42,
        )

        # Convert labels
        labels = [Label.POSITIVE if val == 1 else Label.NEGATIVE for val in y.tolist()]
        gpu_lr.fit(activations, labels)
        gpu_probs = gpu_lr.predict_proba(activations).cpu().numpy()

        # Results should be similar (not exact due to different optimizers)
        # Check that predictions are in the same direction
        sklearn_preds = sklearn_probs.argmax(axis=1)
        gpu_preds = gpu_probs.argmax(axis=1)

        accuracy = (sklearn_preds == gpu_preds).mean()
        assert accuracy > 0.8, (
            f"GPU and sklearn predictions differ too much: {accuracy:.2f}"
        )
